{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://elitedatascience.com/keras-tutorial-deep-learning-in-python\n",
    "\n",
    "import numpy\n",
    "import theano\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Simple linear stack of neural network layer\n",
    "# Perfect for the type of FF CNN\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPool2D\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "plt.imshow(X_train[172])\n",
    "print(y_train[172])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using the theano backend, you must explicity declare a dimension for the *depth* of the input image. for example, a full-color image with all 3 **RGB** channels will have a depth of 3.\n",
    "\n",
    "Our MNIST images only have a depth of 1, but we must explicity declare that.\n",
    "\n",
    "In other words, we want to transform our dataset from having shape (n, width, height) to (n, depth, width, height)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0],1,28,28)\n",
    "X_test = X_test.reshape(X_test.shape[0],1,28,28)\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The final preprocessing step for the input data is to convert our data type to **float32** and normalize our data values to the range [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our input data are ready for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess class labels for Keras\n",
    "\n",
    "Next, let's take a look at the shape of our class label data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should have 10 diffirent classes, one for each digit, but it looks like we only have a 1-dimensional array. Let's take a look at the labels for the first 10 training samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there's the problem. The y_train and y_test data are nit split into 10 distinct class labels, but rather are represented as a single array ith the class values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fix this easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 1-dimensional class arrays to 10-dimensional class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_test =np_utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model architecture\n",
    "\n",
    "Now we're ready to define our model architecture. In actual R&D work, researchers will spend considerable amount of time studying model architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by declaring a sequential model format:\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to fix problem\n",
    "#https://stackoverflow.com/questions/41651628/negative-dimension-size-caused-by-subtracting-3-from-1-for-conv2d\n",
    "\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\", input_shape=(1, 28, 28...)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Next, we declare the input layer:\n",
    "model.add(Convolution2D(32, 3, 3, activation='relu', input_shape=(1,28,28)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input shape parameter should be the shape of 1 sample. In this case, it's the same (1,28,28) that corresponds to the (depth, width, height) of each digit image.\n",
    "\n",
    "32, 3 ,3 correnspond to the number of convolution filters to use, the number of rows in each convolution kernel, and the number of columns in each convolution kernel, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 32, 26, 26)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\")`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "model.add(Convolution2D(32, 3, 3, activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dropout - method for regularizing our model in order to prevent overfitting. (https://www.quora.com/How-does-the-dropout-method-work-in-deep-learning-And-why-is-it-claimed-to-be-an-effective-trick-to-improve-your-network)\n",
    "* MaxPolling2D - way to reduce the numer of parameterss in our model by sliding a 2x2 polling filter across the previous layer and taking the max of the 4 values in the 2x2 filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete our model architecture, let's add a fully connected layer and then the output layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 32, 26, 26)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 32, 24, 24)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 32, 12, 12)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32, 12, 12)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               589952    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 600,810\n",
      "Trainable params: 600,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Dense Layers, the first parameter is the output size of the layer. Keras automatically handles the connections between layers.\n",
    "\n",
    "Note that the final layer has an output size of 10, corresponding to the 10 classes of digits.\n",
    "\n",
    "Also note that the weights from the Convolution layers must be flattened (made 1-dimensional) before passing them to the fully connected Dense layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile model\n",
    "\n",
    "We just need to compile the model and we'll be ready to train it. When we compile the model, we declare the loss function and the optimizer (SGD, Adam, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit model on training data.\n",
    "\n",
    "To fit the model, all we have to do is to declare the batch size and number of epochs to train for, then pass in our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 369s 6ms/step - loss: 0.2093 - acc: 0.9361\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 0.0881 - acc: 0.9739\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 386s 6ms/step - loss: 0.0681 - acc: 0.9794\n",
      "Epoch 4/10\n",
      "26688/60000 [============>.................] - ETA: 3:29 - loss: 0.0565 - acc: 0.9835"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, batch_size=32, nb_epoch=10, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
